2023-02-20T09:12:37,182 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:12:37,182 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:12:37,216 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:12:37,216 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:12:37,545 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220090829326-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:12:37,545 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220090829326-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:12:37,556 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220090829326-shutdown.cfg",
  "modelCount": 1,
  "created": 1676855309326,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:12:37,556 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220090829326-shutdown.cfg",
  "modelCount": 1,
  "created": 1676855309326,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:12:37,564 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220090829326-shutdown.cfg
2023-02-20T09:12:37,564 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220090829326-shutdown.cfg
2023-02-20T09:12:37,566 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220090829326-shutdown.cfg validated successfully
2023-02-20T09:12:37,566 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220090829326-shutdown.cfg validated successfully
2023-02-20T09:12:38,193 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:12:38,193 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:12:38,193 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:12:38,193 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:12:38,194 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:12:38,194 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:12:38,194 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:12:38,194 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:12:38,194 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:12:38,194 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:12:38,208 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:12:38,208 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:12:38,210 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:12:38,210 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:12:38,270 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:12:38,270 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:12:38,270 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:12:38,270 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:12:38,271 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:12:38,271 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:12:38,272 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:12:38,272 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:12:38,272 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:12:38,272 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:12:38,319 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - Traceback (most recent call last):
2023-02-20T09:12:38,320 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py", line 16, in <module>
2023-02-20T09:12:38,320 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.metrics.metric_cache_yaml_impl import MetricsCacheYamlImpl
2023-02-20T09:12:38,320 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/metrics/metric_cache_yaml_impl.py", line 5, in <module>
2023-02-20T09:12:38,321 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     import yaml
2023-02-20T09:12:38,321 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2023-02-20T09:12:38,324 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:12:38,324 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:12:38,324 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:12:38,324 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:12:38,325 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:12:38,325 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:12:38,326 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:12:38,326 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:12:38,326 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:12:38,326 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:12:38,466 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:12:38,466 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:12:38,542 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855558
2023-02-20T09:12:38,542 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.4412803649902|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855558
2023-02-20T09:12:38,543 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855558
2023-02-20T09:12:38,543 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855558
2023-02-20T09:12:38,543 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:19579.92578125|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855558
2023-02-20T09:12:38,543 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11118.3515625|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855558
2023-02-20T09:12:38,544 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:40.2|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855558
2023-02-20T09:13:38,471 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:13:38,471 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:13:38,552 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855618
2023-02-20T09:13:38,553 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.4395065307617|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855618
2023-02-20T09:13:38,553 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855618
2023-02-20T09:13:38,553 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855618
2023-02-20T09:13:38,553 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:19344.7109375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855618
2023-02-20T09:13:38,553 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11345.7421875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855618
2023-02-20T09:13:38,554 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:41.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855618
2023-02-20T09:14:38,470 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:14:38,470 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:14:38,550 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855678
2023-02-20T09:14:38,550 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.4413566589355|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855678
2023-02-20T09:14:38,550 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855678
2023-02-20T09:14:38,550 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855678
2023-02-20T09:14:38,550 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:19553.5546875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855678
2023-02-20T09:14:38,551 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11141.0703125|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855678
2023-02-20T09:14:38,551 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:40.3|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855678
2023-02-20T09:15:22,852 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:15:22,852 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:15:22,886 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:15:22,886 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:15:23,193 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220091459665-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:15:23,193 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220091459665-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:15:23,209 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220091459665-shutdown.cfg",
  "modelCount": 1,
  "created": 1676855699665,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:15:23,209 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220091459665-shutdown.cfg",
  "modelCount": 1,
  "created": 1676855699665,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:15:23,216 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220091459665-shutdown.cfg
2023-02-20T09:15:23,216 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220091459665-shutdown.cfg
2023-02-20T09:15:23,217 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220091459665-shutdown.cfg validated successfully
2023-02-20T09:15:23,217 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220091459665-shutdown.cfg validated successfully
2023-02-20T09:15:23,781 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:15:23,781 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:15:23,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:15:23,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:15:23,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:15:23,782 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:15:23,782 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:15:23,782 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:15:23,783 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:15:23,783 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:15:23,795 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:15:23,795 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:15:23,797 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:15:23,797 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:15:23,856 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:15:23,856 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:15:23,857 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:15:23,857 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:15:23,857 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:15:23,857 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:15:23,858 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:15:23,858 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:15:23,858 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:15:23,858 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:15:23,897 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - Traceback (most recent call last):
2023-02-20T09:15:23,898 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py", line 16, in <module>
2023-02-20T09:15:23,898 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.metrics.metric_cache_yaml_impl import MetricsCacheYamlImpl
2023-02-20T09:15:23,898 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/metrics/metric_cache_yaml_impl.py", line 5, in <module>
2023-02-20T09:15:23,898 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     import yaml
2023-02-20T09:15:23,899 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2023-02-20T09:15:23,903 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:15:23,903 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:15:23,903 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:15:23,903 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:15:23,904 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:15:23,904 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:15:23,904 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:15:23,904 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:15:23,904 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:15:23,904 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:15:24,044 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:15:24,044 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:15:24,125 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855724
2023-02-20T09:15:24,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.4412040710449|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855724
2023-02-20T09:15:24,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855724
2023-02-20T09:15:24,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855724
2023-02-20T09:15:24,126 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:19565.16015625|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855724
2023-02-20T09:15:24,126 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11128.671875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855724
2023-02-20T09:15:24,126 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:40.3|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855724
2023-02-20T09:16:24,046 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:16:24,046 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:16:24,128 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855784
2023-02-20T09:16:24,128 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.4454574584961|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855784
2023-02-20T09:16:24,128 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855784
2023-02-20T09:16:24,129 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855784
2023-02-20T09:16:24,129 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:19525.80859375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855784
2023-02-20T09:16:24,129 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11167.34375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855784
2023-02-20T09:16:24,129 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:40.4|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855784
2023-02-20T09:17:24,045 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:17:24,045 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:17:24,125 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855844
2023-02-20T09:17:24,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.4422454833984|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855844
2023-02-20T09:17:24,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855844
2023-02-20T09:17:24,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855844
2023-02-20T09:17:24,144 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:19409.34375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855844
2023-02-20T09:17:24,145 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11281.66796875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855844
2023-02-20T09:17:24,145 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:40.8|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676855844
2023-02-20T09:20:02,141 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:20:02,141 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:20:02,170 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:20:02,170 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:20:02,766 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220091807525-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:20:02,766 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220091807525-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:20:02,781 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220091807525-shutdown.cfg",
  "modelCount": 1,
  "created": 1676855887525,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:20:02,781 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220091807525-shutdown.cfg",
  "modelCount": 1,
  "created": 1676855887525,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:20:02,788 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220091807525-shutdown.cfg
2023-02-20T09:20:02,788 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220091807525-shutdown.cfg
2023-02-20T09:20:02,789 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220091807525-shutdown.cfg validated successfully
2023-02-20T09:20:02,789 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220091807525-shutdown.cfg validated successfully
2023-02-20T09:20:03,396 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:20:03,396 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:20:03,397 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:20:03,397 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:20:03,397 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:20:03,397 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:20:03,397 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:20:03,397 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:20:03,397 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:20:03,397 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:20:03,408 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:20:03,408 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:20:03,409 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:20:03,409 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:20:03,477 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:20:03,477 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:20:03,477 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:20:03,477 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:20:03,479 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:20:03,479 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:20:03,479 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:20:03,479 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:20:03,480 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:20:03,480 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:20:03,684 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:20:03,684 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:20:03,683 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - Traceback (most recent call last):
2023-02-20T09:20:03,684 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2023-02-20T09:20:03,685 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2023-02-20T09:20:03,685 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_loader.py", line 13, in <module>
2023-02-20T09:20:03,685 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.service import Service
2023-02-20T09:20:03,685 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/service.py", line 10, in <module>
2023-02-20T09:20:03,686 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2023-02-20T09:20:03,686 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 13, in <module>
2023-02-20T09:20:03,686 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     import torch
2023-02-20T09:20:03,687 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2023-02-20T09:20:03,692 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:20:03,692 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:20:03,692 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:20:03,692 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:20:03,694 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:20:03,694 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:20:03,694 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:20:03,694 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:20:03,694 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:20:03,694 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:20:03,782 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856003
2023-02-20T09:20:03,782 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.4612693786621|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856003
2023-02-20T09:20:03,783 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856003
2023-02-20T09:20:03,783 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856003
2023-02-20T09:20:03,783 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18749.08203125|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856003
2023-02-20T09:20:03,784 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11928.65234375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856003
2023-02-20T09:20:03,784 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:42.8|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856003
2023-02-20T09:21:05,038 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:21:05,038 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:21:05,068 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:21:05,068 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:21:05,381 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220092058608-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:21:05,381 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220092058608-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:21:05,396 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220092058608-shutdown.cfg",
  "modelCount": 1,
  "created": 1676856058608,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:21:05,396 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220092058608-shutdown.cfg",
  "modelCount": 1,
  "created": 1676856058608,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:21:05,403 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220092058608-shutdown.cfg
2023-02-20T09:21:05,403 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220092058608-shutdown.cfg
2023-02-20T09:21:05,403 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220092058608-shutdown.cfg validated successfully
2023-02-20T09:21:05,403 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220092058608-shutdown.cfg validated successfully
2023-02-20T09:21:05,952 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:21:05,952 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:21:05,952 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:21:05,952 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:21:05,952 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:21:05,952 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:21:05,953 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:21:05,953 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:21:05,953 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:21:05,953 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:21:05,967 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:21:05,967 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:21:05,968 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:21:05,968 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:21:06,027 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:21:06,027 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:21:06,027 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:21:06,027 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:21:06,029 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:21:06,029 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:21:06,029 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:21:06,029 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:21:06,030 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:21:06,030 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:21:06,097 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - Traceback (most recent call last):
2023-02-20T09:21:06,098 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2023-02-20T09:21:06,098 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2023-02-20T09:21:06,099 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_loader.py", line 13, in <module>
2023-02-20T09:21:06,099 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.service import Service
2023-02-20T09:21:06,099 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/service.py", line 10, in <module>
2023-02-20T09:21:06,099 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2023-02-20T09:21:06,099 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 13, in <module>
2023-02-20T09:21:06,100 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     import torch
2023-02-20T09:21:06,100 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2023-02-20T09:21:06,104 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:21:06,104 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:21:06,104 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:21:06,104 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:21:06,106 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:21:06,106 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:21:06,106 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:21:06,106 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:21:06,106 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:21:06,106 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:21:06,225 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:21:06,225 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:21:06,307 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856066
2023-02-20T09:21:06,307 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.463264465332|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856066
2023-02-20T09:21:06,307 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856066
2023-02-20T09:21:06,308 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856066
2023-02-20T09:21:06,308 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18728.18359375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856066
2023-02-20T09:21:06,308 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11950.390625|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856066
2023-02-20T09:21:06,308 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:42.8|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856066
2023-02-20T09:28:19,031 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:28:19,031 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-02-20T09:28:19,064 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:28:19,064 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-02-20T09:28:19,368 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220092140391-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:28:19,368 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages
Current directory: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources
Temp directory: /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T/
Metrics config path: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 16
Max heap size: 8192 M
Python executable: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python
Config file: logs/config/20230220092140391-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Initial Models: helmet_detection.mar
Log dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Metrics dir: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 16
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/helmet_yolov5_torchserve/ressources/model_store
Model config: N/A
2023-02-20T09:28:19,383 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220092140391-shutdown.cfg",
  "modelCount": 1,
  "created": 1676856100392,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:28:19,383 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20230220092140391-shutdown.cfg",
  "modelCount": 1,
  "created": 1676856100392,
  "models": {
    "helmet_detection": {
      "0.1": {
        "defaultVersion": true,
        "marName": "helmet_detection.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-02-20T09:28:19,389 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220092140391-shutdown.cfg
2023-02-20T09:28:19,389 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20230220092140391-shutdown.cfg
2023-02-20T09:28:19,390 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220092140391-shutdown.cfg validated successfully
2023-02-20T09:28:19,390 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20230220092140391-shutdown.cfg validated successfully
2023-02-20T09:28:19,971 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:28:19,971 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 0.1 for model helmet_detection
2023-02-20T09:28:19,971 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:28:19,971 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:28:19,972 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:28:19,972 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 0.1 for model helmet_detection
2023-02-20T09:28:19,972 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:28:19,972 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model helmet_detection loaded.
2023-02-20T09:28:19,972 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:28:19,972 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: helmet_detection, count: 1
2023-02-20T09:28:19,983 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:28:19,983 [DEBUG] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/bin/python, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /var/folders/cm/pzskfkq17lq6vp0d73nrg1dw0000gq/T//.ts.sock.9000, --metrics-config, /Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-02-20T09:28:19,984 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:28:19,984 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: KQueueServerSocketChannel.
2023-02-20T09:28:20,045 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:28:20,045 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-02-20T09:28:20,045 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:28:20,045 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: KQueueServerSocketChannel.
2023-02-20T09:28:20,046 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:28:20,046 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-02-20T09:28:20,046 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:28:20,046 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: KQueueServerSocketChannel.
2023-02-20T09:28:20,046 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:28:20,046 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-02-20T09:28:20,113 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - Traceback (most recent call last):
2023-02-20T09:28:20,113 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_service_worker.py", line 17, in <module>
2023-02-20T09:28:20,114 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.model_loader import ModelLoaderFactory
2023-02-20T09:28:20,114 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/model_loader.py", line 13, in <module>
2023-02-20T09:28:20,114 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.service import Service
2023-02-20T09:28:20,114 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/service.py", line 10, in <module>
2023-02-20T09:28:20,115 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2023-02-20T09:28:20,115 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -   File "/Users/juanl/Documents/01-career/001-peachproject/04-model_matching/tutorial-examples/torch-env/lib/python3.8/site-packages/ts/protocol/otf_message_handler.py", line 13, in <module>
2023-02-20T09:28:20,115 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG -     import torch
2023-02-20T09:28:20,115 [WARN ] W-9000-helmet_detection_0.1-stderr MODEL_LOG - ModuleNotFoundError: No module named 'torch'
2023-02-20T09:28:20,120 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:28:20,120 [INFO ] W-9000-helmet_detection_0.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stderr
2023-02-20T09:28:20,120 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:28:20,122 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:28:20,120 [INFO ] W-9000-helmet_detection_0.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-helmet_detection_0.1-stdout
2023-02-20T09:28:20,122 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stderr
2023-02-20T09:28:20,123 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:28:20,123 [WARN ] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-helmet_detection_0.1-stdout
2023-02-20T09:28:20,123 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:28:20,123 [ERROR] W-9000-helmet_detection_0.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:155) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:306) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:181) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-02-20T09:28:20,234 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:28:20,234 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:28:20,306 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856500
2023-02-20T09:28:20,306 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.5222625732422|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856500
2023-02-20T09:28:20,306 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856500
2023-02-20T09:28:20,307 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856500
2023-02-20T09:28:20,307 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18919.9921875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856500
2023-02-20T09:28:20,307 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11779.24609375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856500
2023-02-20T09:28:20,308 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:42.3|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856500
2023-02-20T09:29:20,238 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:29:20,238 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:29:20,323 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856560
2023-02-20T09:29:20,324 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.5178604125977|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856560
2023-02-20T09:29:20,324 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856560
2023-02-20T09:29:20,324 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856560
2023-02-20T09:29:20,324 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18866.85546875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856560
2023-02-20T09:29:20,324 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11830.671875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856560
2023-02-20T09:29:20,324 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:42.4|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856560
2023-02-20T09:30:20,236 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:30:20,236 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:30:20,315 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856620
2023-02-20T09:30:20,315 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.6272010803223|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856620
2023-02-20T09:30:20,315 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856620
2023-02-20T09:30:20,315 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856620
2023-02-20T09:30:20,316 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18237.6796875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856620
2023-02-20T09:30:20,316 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:12479.44921875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856620
2023-02-20T09:30:20,316 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.3|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856620
2023-02-20T09:31:20,237 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:31:20,237 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:31:20,313 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856680
2023-02-20T09:31:20,313 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:631.5957374572754|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856680
2023-02-20T09:31:20,313 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856680
2023-02-20T09:31:20,314 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856680
2023-02-20T09:31:20,314 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:18226.01953125|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856680
2023-02-20T09:31:20,314 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:12489.68359375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856680
2023-02-20T09:31:20,314 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:44.4|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856680
2023-02-20T09:32:20,240 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:32:20,240 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:32:20,320 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856740
2023-02-20T09:32:20,320 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:631.5626602172852|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856740
2023-02-20T09:32:20,320 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856740
2023-02-20T09:32:20,320 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856740
2023-02-20T09:32:20,320 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18210.56640625|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856740
2023-02-20T09:32:20,320 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:12506.40234375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856740
2023-02-20T09:32:20,320 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.4|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856740
2023-02-20T09:33:20,238 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:33:20,238 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:33:20,317 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856800
2023-02-20T09:33:20,317 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:631.5237007141113|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856800
2023-02-20T09:33:20,317 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856800
2023-02-20T09:33:20,317 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856800
2023-02-20T09:33:20,317 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:18074.87890625|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856800
2023-02-20T09:33:20,317 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:12626.78125|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856800
2023-02-20T09:33:20,317 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:44.8|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856800
2023-02-20T09:34:20,237 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:34:20,237 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:34:20,318 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856860
2023-02-20T09:34:20,318 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:631.6255149841309|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856860
2023-02-20T09:34:20,318 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856860
2023-02-20T09:34:20,318 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856860
2023-02-20T09:34:20,318 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:18509.9921875|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856860
2023-02-20T09:34:20,319 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:12193.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856860
2023-02-20T09:34:20,319 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:43.5|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856860
2023-02-20T09:35:20,238 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:35:20,238 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-02-20T09:35:20,319 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856920
2023-02-20T09:35:20,319 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:631.585132598877|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856920
2023-02-20T09:35:20,319 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:23.694202423095703|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856920
2023-02-20T09:35:20,319 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:3.6|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856920
2023-02-20T09:35:20,319 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:18387.1484375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856920
2023-02-20T09:35:20,319 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:12316.12109375|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856920
2023-02-20T09:35:20,320 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:43.9|#Level:Host|#hostname:juanlEMD6R.vmware.com,timestamp:1676856920
